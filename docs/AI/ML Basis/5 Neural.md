# Chapter 5: 神经网络

## 1. 神经元模型

## 2. 感知机和多层网络

给定训练数据集，权重 $w_i$ $(i=1,2,\ldots,n)$ 以及阈值 $\theta$ 可通过学习得到。阈值 $\theta$ 可以看作一个固定输入为 $-1.0$ 的“哑结点”(dummy node)所对应的连接权重 $w_{n+1}$。这样，权重和阈值的学习就统一为权重的学习。

感知机的学习规则非常简单：对训练样例 $(\boldsymbol{x},y)$，若当前感知机的输出为 $\hat{y}$，则感知机权重将按如下方式调整：

$$\begin{aligned}
    w_i &\leftarrow w_i + \Delta w_i,\\ \Delta w_i &= \eta(y-\hat{y})x_i.
\end{aligned}$$

其中 $\eta \in (0,1)$ 称为学习率(learning rate)。

从上式可以看出：

- 当感知机对训练样例预测正确时，即 $y=\hat{y}$，感知机不会发生变化
- 当预测错误时，则通过调整权重来纠正错误

感知机只有输出层神经元进行激活函数处理，也就是只有一层**功能神经元/Functional Neuron**，其学习能力十分有限。事实上，与、或、非问题都是线性可分问题，而异或问题是非线性可分问题。可以证明，如果两类模式是线性可分的，那么存在一个线性超平面可以将其分开。对于线性可分问题，感知机的学习过程一定会收敛/Converge，否则感知机学习过程将会发生震荡/Fluctuation。

要解决非线性可分问题，就需要使用多层功能神经元，使用简单的两层感知机就可以解决异或问题，其中输入层与输出层之间还有一层神经元，其被称为隐层或者隐含层/Hidden Layer。**隐含层神经元和输出层神经元都是具有激活函数功能的功能神经元**。

<img class="center-picture" src="../assets/5-1.png" alt="5-1" width=900 />

更一般的，常见的神经网络具有层级结构，每层神经元和下面一层神经元完全互连，神经元之间不存在同层连接，也不存在跨层连接.这样的神经网络结构通常称为**多层前馈神经网络/Multi-layer Feedforward Neural Networks**，其中其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。

神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权/Connection Weight 以及每个功能神经元的阈值；换言之，神经网络学习到的东西，蕴涵在连接权与阈值中。

## 3. 误差反向传播算法


给定训练数据集 $D = \{(\boldsymbol{x}_1, \boldsymbol{y}_1), (\boldsymbol{x}_2, \boldsymbol{y}_2), \cdots, (\boldsymbol{x}_m, \boldsymbol{y}_m)\}$，其中 $\boldsymbol{x}_i \in \mathbb{R}^d$，$\boldsymbol{y}_i \in \mathbb{R}^l$，即输入示例由 $d$ 个属性描述，输出 $l$ 维实值向量。

对于一个拥有 $d$ 个输入节点、$l$ 个输出节点、$q$ 个隐层节点的多层前馈网络：

- 输入层第 $i$ 个神经元与隐层第 $h$ 个神经元之间的连接权为 $v_{ih}$；
- 隐层第 $h$ 个神经元与输出层第 $j$ 个神经元之间的连接权为 $w_{hj}$；
- 隐层第 $h$ 个神经元接收到的输入为 $\alpha_h = \sum\limits_{i=1}^d v_{ih}x_i$；
- 输出层第 $j$ 个神经元接收到的输入为 $\beta_j = \sum\limits_{h=1}^q w_{hj}b_h$；

其中 $b_h$ 为隐层第 $h$ 个神经元的输出。假设隐层和输出层神经元都使用 Sigmoid 函数。

对训练样例 $(\boldsymbol{x}_k,\boldsymbol{y}_k)$，假设神经网络的输出为 $\hat{\boldsymbol{y}}_k = (\hat{y}_1^k,\hat{y}_2^k,\ldots,\hat{y}_l^k)$，即 

$$\hat{y}_j^k = f(\beta_j - \theta_j)$$ 

其中 $f$ 为 Sigmoid 函数。则网络在 $(\boldsymbol{x}_k,\boldsymbol{y}_k)$ 上的均方误差为 

$$E_k = \frac{1}{2}\sum_{j=1}^l(\hat{y}_j^k - y_j^k)^2$$

反向传播算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，以对隐层和输出层之间的链接权为例，对上面的均方误差 $E_k$，给定学习率 $\eta$，有：

$$\Delta w_{hj} = -\eta\frac{\partial E_k}{\partial w_{hj}}$$

<!--
通过链式法则，可得：

$$\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial w_{hj}}$$ (5.7)

其中：
$$\frac{\partial \beta_j}{\partial w_{hj}} = b_h$$ (5.8)

Sigmoid函数具有一个很好的性质：

$$f'(x) = f(x)(1-f(x))$$ (5.9)

最终可得BP算法中关于 $w_{hj}$ 的更新式：

$$\Delta w_{hj} = \eta g_j b_h$$ (5.11)

类似可得：
$$\Delta \theta_j = -\eta g_j$$ (5.12)
$$\Delta v_{ih} = \eta e_h x_i$$ (5.13)
$$\Delta \gamma_h = -\eta e_h$$ (5.14)

其中 $e_h$ 的计算公式为：

$$e_h = b_h(1-b_h)\sum_{j=1}^l w_{hj}g_j$$ (5.15)

### 3.3 算法流程

1. 在 $(0,1)$ 范围内随机初始化网络中所有连接权和阈值
2. repeat
3. for all $(x_k,y_k) \in D$ do
4. 根据当前参数计算当前样本的输出 $\hat{y}_k$
5. 根据式(5.10)计算输出层神经元的梯度项 $g_j$
6. 根据式(5.15)计算隐层神经元的梯度项 $e_h$
7. 根据式(5.11)-(5.14)更新连接权 $w_{hj}$, $v_{ih}$ 与阈值 $\theta_j$, $\gamma_h$
8. end for
9. until 达到停止条件

学习率 $\eta$ 控制着算法每一轮迭代中的更新步长。若太大则容易振荡，太小则收敛速度又会过慢。有时为了精细调节，可对式(5.11)与(5.12)使用 $\eta_1$，对式(5.13)与(5.14)使用 $\eta_2$，两者未必相同。 -->


## 4. 全局最小与局部最小

现实任务中，人们经常使用一下策略来试图跳出局部最小，从而进一步接近全局最小：

- 以多组不同参数值初始化多个神经网络，按照标准方法训练后，取其中误差最小的解作为最终参数。这相当于从多个不同点开始搜索，这样就可能陷入不同的局部最小，从中选择有可能获得更接近全局最小的结果。
- 使用模拟退火/Simulated Annealing 技术，即在每一步都以一定的概率接受比当前解更差的结果，从而有助于跳出局部最小（也有可能跳出全局最小）。在每步迭代过程中，接收次优解的概率要随着时间的推移而逐渐降低，从而保证算法稳定。
- 使用随机梯度下降/Stochastic Gradient Descent。与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加入了随机因素。即便陷入局部极小点，它计算出的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索。

遗传算法也常用来训练神经网络以更好逼近全局最小，但是上述技术大多数都是启发式，理论上缺乏一定保障。

## 5. 其他常见神经网络

### 5.1 RBF 网络

使用径向基函数/Radial Basis Function 作为隐层神经元的激活函数，输出层则是对隐层神经元输出的线性组合。假定输入为 $d$ 维向量 $\boldsymbol{x}$，则 RBF 网络可以表示为 

$$\varphi(\boldsymbol{x}) = \sum_{i=1}^{q}w_i\rho(\boldsymbol{x}, \boldsymbol{c}_i)$$

<!-- 其中：
- $q$ 为隐层神经元个数
- $c_i$ 和 $w_i$ 分别是第 $i$ 个隐层神经元所对应的中心和权重
- $\rho(x,c_i)$ 是径向基函数，是样本 $x$ 到数据中心 $c_i$ 之间欧氏距离的单调函数
- 常用的高斯径向基函数形如：
  $$\rho(x,c_i) = e^{-\beta_i\|x-c_i\|^2}$$

[Park and Sandberg, 1991] 证明，具有足够多隐层神经元的 RBF 网络能以任意精度逼近任意连续函数。

训练过程通常采用两步：
1. 确定神经元中心 $c_i$，常用方法包括随机采样、聚类等
2. 利用 BP 算法来训练参数 $w_i$ 和 $\beta_i$ -->

### 5.2 ART 网络

竞争性学习/Comeptitive Learning 是神经网络中的一种常用的一种无监督学习策略

ART/自适应偕振理论/Adaptive Resonance Theory 网络是竞争性学习的重要代表，其结构包括比较层、识别层、识别阈值和重置模块。工作机制如下：

<!-- 竞争型学习 (competitive learning) 是神经网络中一种常用的无监督学习策略。其特点是：
- 网络的输出神经元相互竞争
- 每一时刻仅有一个竞争获胜的神经元被激活
- 其他神经元的状态被抑制
- 这种机制称为"胜者通吃" (winner-take-all) 原则



工作机制：
1. 识别层每个神经元对应一个模式类
2. 神经元数目可在训练过程中动态增加
3. 在接收到比较层的输入信号后，识别层神经元之间相互竞争
4. 计算输入向量与各个识别层神经元对应的模式类的相似度
5. 距离最小者获胜
6. 若相似度大于识别阈值，则当前输入样本将归为该类
7. 同时更新网络连接权
8. 若相似度不大于阈值，则在识别层增设新神经元

ART的重要特点：
- 能很好地解决"可塑性-稳定性窘境" (stability-plasticity dilemma)
  - 可塑性指神经网络要有学习新知识的能力
  - 稳定性指神经网络在学习新知识时要保持对旧知识的记忆
- 适合增量学习 (incremental learning) 和在线学习 (online learning)

发展：
- 早期ART网络只能处理布尔型输入数据
- 后来发展出：
  - 可处理实值输入的ART2网络
  - 结合模糊处理的Fuzzy-ART网络
  - 可进行监督学习的ARTMAP网络 
-->

### 5.3 SOM 网络

<!-- SOM (Self-Organizing Map，自组织映射) 网络 [Kohonen, 1982] 是一种竞争学习型的无监督神经网络。主要特点：
- 将高维输入数据映射到低维空间（通常为二维）
- 同时保持输入数据在高维空间的拓扑结构

网络结构：
- 输出层神经元以矩形方式排列在二维空间中
- 每个神经元都拥有一个权向量
- 网络在接收输入向量后，会自动定位输出层状态最接近的神经元位置

训练过程：
1. 接收一个训练样本后，每个输出层神经元计算该样本与自身权向量之间的距离
2. 距离最近的神经元成为竞争获胜者（最佳匹配单元/best matching unit）
3. 最佳匹配单元及其邻近神经元的权向量被调整，以使这些权向量与当前输入样本的距离缩小
4. 不断迭代这个过程直至收敛 -->

### 5.4 级联相关网络

结构自适应网络将网络结构也当作学习的目标之一，希望能在训练过程中找到最符合数据特点的网络结构。级联相关网络/Cascade-Correlation Network 是结构自适应网络的代表之一。

级联相关网络有两个主要成分，级联和相关。级联是指建立层次连接的层次结构。在开始训练的时候，网络只有输入层和输出层，处于最小拓扑结构，随着训练的进行，新的隐层神经元逐渐加入，从而创建起层次结构。当新的隐层神经元加入时，其输出端链接权重是固定的。相关是指通过最大化新神经元的输出与网络误差之间的相关性来训练相关的参数。

级联相关网络不需要设置网络层数、隐层神经元数目，训练速度较快，但是数据规模较小的时候可能会陷入过拟合。

### 5.5 Elman 网络

递归神经网络/Recurrent Neural Network 允许网络中出现环形结构，从而可以让一些神经元的输出反馈回来作为输入信号。这样的结构和信息反馈过程，使得网络在某时刻的输出状态不仅仅与当前时刻的输入有关，还与前一时刻的网络状态有关，从而可以处理与时间相关的动态变化。

### 5.6 Boltzmann 机

有一类神经网络中的模型是为网络状态定义一个能量，当能量最小的时候网络达到理想状态，网络的训练就是在最小化这个能量函数。Boltzmann 机是这类神经网络的代表之一。

## 6. 深度学习



