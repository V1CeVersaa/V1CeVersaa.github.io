# Chapter 1: Introduction

## 1.3 强化学习要素

除了智能体/Agent 和环境之外，强化学习系统有四个元素：策略/Policy、收益信号/Reward Signal、价值函数/Value Function，以及一个可选的对环境建立的模型/Model of the environment。

策略定义了智能体在特定时间的行为方式。大致来说，策略是智能体感知的环境状态到动作的映射。因为策略本身就足以决定行为，因此策略是强化学习的核心。在一般情况下，策略可能是环境所在状态和智能体采取的动作的随机函数。

收益信号定义了强化学习问题的目标。在每一个时间步，环境向智能体发送一个称为收益/Reward 的数字。智能体的唯一目标是最大化长期收益，因此收益信号决定了什么事件是好的，什么事件是坏的。因此收益信号是改变策略的重要基础，如果策略动作产生了低收益，那么策略可能会在未来改变，以选择其他动作。在一般情况下，收益信号可能是环境状态和智能体动作的随机函数。

价值函数表示从长远的角度看，什么是好的。简单来说，一个状态的价值是一个智能体从当前状态开始，对将来累积的总共收益的期望。尽管收益定义了环境状态直接/immediate、即时/intrinsic 的吸引力，但是价值解释了接下来所有可能状态的长期期望。价值评估方法是几乎所有强化学习算法中最重要的组成部分。

对环境建立的模型是对环境反应模式的模拟，更一般的讲是允许对外部环境的行为进行推断/Allows inferences to be made about how the environment will behave。环境模型会被用于做规划。使用环境模型和规划来解决强化学习问题的方法被称为有模型方法，简单的无模型方法则是直接进行试错。

## 1.4 局限与适用范围

强化学习严重依赖状态这个概念，其作为策略和价值函数的输入，并且也作为环境模型的输入输出。但是本书并不处理构造、改变或者学习状态信号的问题。

本书中提到的大多数强化学习算法都围绕着估计价值函数构造，但是这并不是解决强化学习问题的必由之路。比如遗传算法、遗传规划、模拟退火和其他的优化方法都不直接估计价值函数，这些方法使用多种静态策略，每一个静态策略分别和独立的环境进行长期交互，这些方法选择收益最多的策略及其变种产生下一代的策略，我们将这种方法称为**进化方法/Evolutionary Methods**。

我们关注的强化学习方法是在环境互动中学习的一类方法，但是进化方法显然不是。进化方法忽视了强化学习中的一些有用的结构，比如忽略了所求策略是状态到动作的函数这一事实，也没有注意到个体在生命周期都经历过哪些状态，采取了哪些动作。如果一个策略胜出，那么整个策略的所有行为都会被认为是有效的，而不管有哪些特殊的行为直接导致了胜利。简而言之，学习值函数利用了在交互过程中可用的信息。



