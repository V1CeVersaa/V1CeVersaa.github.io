# Chapter 8: 遗憾界

在介绍遗憾/Regret 这一在线学习的性能评价指标之前，首先回顾一下批量学习对数据的利用方式及采取的评价指标。批量学习算法只关心整个学习过程结束后得到的分类器性能，一种常用的评价指标是超额风险/Excess Risk，该指标可以理解为学习算法最终输出的模型与假设空间内最优模型的风险之差。批量学习算法假设所有的训练数据提前获得，当数据规模非常大时，这种模式计算复杂度高、响应慢，无法用于实时性要求高的场景。与批量学习不同，在线学习考虑数据持续增长的场景，通常利用当前到来的训练样本更新模型。由于学习模式的不同，在线学习利用遗憾来评价算法性能，该指标可以理解为算法在运行过程中产生的模型与假设空间内最优模型的损失之差的求和，因此遗憾更关注模型在整个学习过程中的表现。

## 1. 基本概念

对于批量学习而言，学习器通过数据集 $D_T = \{(\boldsymbol{x}_1,y_1),\cdots,(\boldsymbol{x}_T,y_T)\}$ 学习到模型 $\boldsymbol{w}_{T+1}$，该模式下的算法只关心整个学习过程结束后得到的分类器性能，可以采用超额风险作为评价指标：

$$\mathbb{E}_{(\boldsymbol{x},y)\sim\mathcal{D}}[\ell(\boldsymbol{w}_{T+1}, (\boldsymbol{x},y))] - \min_{\boldsymbol{w}\in\mathcal{W}} \mathbb{E}_{(\boldsymbol{x},y)\sim\mathcal{D}}[\ell(\boldsymbol{w}, (\boldsymbol{x},y))].$$

上式子将模型 $\boldsymbol{w}_{T+1}$ 的风险与假设空间内最优模型的风险相比较，而第 4 章中的泛化误差则是将模型的风险与经验风险相比较。




<!-- 

对于批量学习而言，学习器通过数据集 $D_T = \{(\boldsymbol{x}_1,y_1),\cdots,(\boldsymbol{x}_T,y_T)\}$ 学习到模型 $w_{T+1}$，该模式下的算法只关心整个学习过程结束后得到的分类器性能，可以采用超额风险作为评价指标：

$$\mathbb{E}_{(\boldsymbol{x},y)\sim\mathcal{D}}[\ell(w_{T+1}, (\boldsymbol{x},y))] - \min_{w\in\mathcal{W}} \mathbb{E}_{(\boldsymbol{x},y)\sim\mathcal{D}}[\ell(w, (\boldsymbol{x},y))] \tag{8.1}$$

上式将模型 $w_{T+1}$ 的风险与假设空间内最优模型的风险相比较，而第 4 章中的泛化误差则是将模型的风险与经验风险相比较。

对于在线学习而言，学习器通过数据集 $D_{t-1} = \{(\boldsymbol{x}_1,y_1),\cdots,(\boldsymbol{x}_{t-1},y_{t-1})\}$ 学习到模型 $w_t$，遭受损失 $\ell(w_t, (\boldsymbol{x}_t,y_t))$；然后，学习器将样本 $(\boldsymbol{x}_t,y_t)$ 加入到数据集中，继续更新模型。因此对于在线学习算法，下述的 "序贯超额损失" 更能反映学习过程中算法的表现：

$$\sum_{t=1}^{T} \ell(w_t, (\boldsymbol{x}_t,y_t)) - \min_{w\in\mathcal{W}} \sum_{t=1}^{T} \ell(w, (\boldsymbol{x}_t,y_t)) \tag{8.2}$$

注意到，(8.2) 中 $\ell(w_t, (\boldsymbol{x}_t,y_t))$ 反映了模型 $w_t$ 在样本 $(\boldsymbol{x}_t,y_t)$ 上的损失。由于 $w_t$ 的计算过程与样本 $(\boldsymbol{x}_t,y_t)$ 无关，因此可以直接使用 $\ell(w_t, (\boldsymbol{x}_t,y_t))$ 来衡量性能，而不需要像 (8.1) 一样引入期望操作。更一般而言，在线学习可以被形式化为学习器和对手之间的博弈过程：

- 在每一轮 $t$，学习器从假设空间 $\mathcal{W}$ 选择决策 $w_t$。同时，对手选择一个损失函数 $f_t(\cdot):\mathcal{W} \to \mathbb{R}$；
- 学习器遭受损失 $f_t(w_t)$，并更新模型获得 $t + 1$ 轮的解 $w_{t+1}$。

在线学习的目的是最小化累积的损失。假设算法共执行 $T$ 轮迭代，那么累积损失就是 $\sum_{t=1}^{T} f_t(w_t)$。在线算法通常将在线损失和离线算法的最小损失进行比较，其差值定义为 遗憾（regret）：

$$\text{regret} = \sum_{t=1}^{T} f_t(w_t) - \min_{w\in\mathcal{W}} \sum_{t=1}^{T} f_t(w) \tag{8.3}$$

这里 $\min_{w\in\mathcal{W}} \sum_{t=1}^{T} f_t(w)$ 即为离线算法的最小损失，最小化累积损失也就等价于最小化遗憾。在线学习算法希望达到次线性的遗憾，即当 $T \to +\infty$ 时，regret/$T \to 0$。具备次线性遗憾的算法也称为满足 Hannan 一致性（Hannan consistency）[Cesa-Bianchi and Lugosi, 2006]。

根据算法接收反馈的不同，在线学习又可以进一步划分为 完全信息在线学习（full information online learning）和 赌博机在线学习（bandit online learning）。对于完全信息在线学习，学习器可以观测到完整的损失函数 $f_t(\cdot)$，因此可以利用函数的信息（比如梯度）更新模型。对于赌博机在线学习，学习器通常只能观测到损失函数在所决策 $w_t$ 上的值 $f_t(w_t)$，不能观测到损失函数在其他决策上的值。

下面我们通过赛马和赌博机的例子来阐释两者的区别。在赛马比赛中，每一轮选择一匹马，然后所有的马进行比赛；在当前轮比赛结束之后，不仅可以看到所选马的比赛结果，还可以看到其他马的情况。在这种场景下，学习器观测到了完整的损失函数，属于完全信息在线学习。在使用赌博机时，面对多个摇臂，选择其中一个摇动；这样只能获得所选摇臂的反馈，无法观测其他摇臂的值。在这种场景下，学习器只能观测到所选决策的结果，属于赌博机在线学习。
 -->

## 2. 完全信息在线学习

完全信息在线学习

### 2.1 在线凸优化

### 2.2 在线强凸优化

### 2.3 在线凸优化的拓展

## 3. 赌博机在线学习



## 4. 分析实例
