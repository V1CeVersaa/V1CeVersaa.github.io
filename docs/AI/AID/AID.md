# 人工智能引论：复习

!!! Info "无奈的碎碎念"

    这个破书啥都想讲，啥都讲的依托，幸好字体好看，赞美高教社。

    > 如无必要，勿增实体。 —— 奥卡姆剃刀  
    > 如无必要，别写这个破书了。 —— 我

## 机器学习

## 深度学习

### 1. 钩式历史

### 2. 前馈神经网络

前馈神经网络，也叫深度前馈网络或者**多层感知机/Multilayer Perceptron/MLP**。

1. Sigmoid
2. Tanh
3. ReLU/整流线性单元/Restified Linear Unit：$\operatorname*{ReLU}(x) = \max(0, x)$。使用较为普遍，当 $x > 0$ 时，梯度为 1，这就有效缓解了梯度消失问题，但是当 $x < 0$ 时，$\operatorname*{ReLU}$ 的梯度和值都为 0，这就导致神经网络中若干参数的激活值为 0，一方面导致参与分类任务的神经元数目稀缺，另一方面也导致神经元**死亡**，其对应的权重永远不会更新。解决这个问题可以使用 ReLU 的变种 Leaky ReLU 等。
4. Softmax

### 3. 神经网络参数优化

神经网络参数优化是一个监督学习的过程，给定 $n$ 个标注样本数据 $(x_i, y_i)$，其中 $x_i$ 是输入数据，$y_i$ 是标注信息，我们通过损失函数计算模型预测值和真实值之间的差距，通过优化损失函数来优化模型参数。具体而言，模型会利用反向传播算法将损失函数计算所得到的误差从输出端出发，从后向前传递给神经网络的每个单元，通过梯度下降对参数进行优化。

**损失函数/Loss Function**：

1. 均方误差损失函数
2. 交叉熵损失函数：在信息论的视角来看，如果对于同一个随机变量 $\mathtt{x}$ 有两个单独的概率分布 $P$ 和 $Q$，令其交叉熵为 $H(P, Q) = -\mathbb{E}_{\mathtt{x}\sim P}\log Q(x)$。在机器学习的视角来看，其中 $P$ 是 $\mathtt{x}$ 的真实分布，$Q$ 是 $\mathtt{x}$ 的预测分布。交叉熵损失描述了两个概率分布的距离。可以通过最小化交叉熵损失训练神经网络。



**梯度下降**：

**反向传播**：

### 4. 卷积神经网络

**卷积计算**：

**池化**：

- 最大池化
- 平均池化
- k-max 池化

### 5. 循环神经网络

**循环神经网络模型**：

**长短时记忆网络/Long Short-Term Memory**：引入了**内部记忆单元/Internal Memory Cell** 和**门结构/Gate** 对当前时刻输入信息和前序时刻生成信息进行整合和前递，内部记忆单元的信息可以视为对历史信息的累积。符号和术语如下：

- $x_t$：时刻 $t$ 的输入；
- $i_t$：输入门的输出，$i_t = \operatorname*{sigmoid}(W_{xi}x_t + W_{hi}h_{t-1} + b_i)$，其中 $W_{xi}$、$W_{hi}$ 和 $b_i$ 是输入门的权重和偏置；
- $f_t$：遗忘门的输出，$f_t = \operatorname*{sigmoid}(W_{xf}x_t + W_{hf}h_{t-1} + b_f)$，其中 $W_{xf}$、$W_{hf}$ 和 $b_f$ 是遗忘门的权重和偏置；
- $o_t$：输出门的输出，$o_t = \operatorname*{sigmoid}(W_{xo}x_t + W_{ho}h_{t-1} + b_o)$，其中 $W_{xo}$、$W_{ho}$ 和 $b_o$ 是输出门的权重和偏置；
- $c_t$：内部记忆单元的输出，$c_t = f_t \odot c_{t-1} + i_t \odot \operatorname*{tanh}(W_{xc}x_t + W_{hc}h_{t-1} + b_c)$，其中 $W_{xc}$、$W_{hc}$ 和 $b_c$ 是内部记忆单元的权重和偏置，输入门 $i_t$ 控制有多少信息流入当前时刻内部记忆单元 $c_t$，遗忘门 $f_t$ 控制上一时刻内部记忆单元 $c_{t-1}$ 有多少信息流入当前时刻内部记忆单元 $c_t$；
- $h_t$：时刻 $t$ 输入数据的隐式编码，$h_t = o_t \odot \operatorname*{tanh}(c_t)$，输入门、遗忘门和输出门的信息一起参与得到当前时刻的隐式编码 $h_t$；
- $\odot$：逐元素乘法。

<img class="center-picture" src="../assets/4-DL-LSTM.png" alt="illustration" width="550" />

对于 $c_t = f_t \odot c_{t-1} + i_t \odot \operatorname*{tanh}(W_{xc}x_t + W_{hc}h_{t-1} + b_c)$，可以看到

$$
\frac{\partial c_t}{\partial c_{t-1}} = f_t + \frac{\partial f_t}{\partial c_{t-1}}c_{t-1} + \cdots \geq f_t
$$

可以看到，LSTM 通过引入门结构，在相邻的时刻之间使用加法进行信息更新，可以避免梯度消失问题。

**门控循环单元/Gated Recurrent Unit**：简化了 LSTM，不使用记忆单元，使用隐藏状态进行信息传递。

- 输入数据 $x_t$、前一时刻隐式编码 $h_{t-1}$；
- 更新门/Update Gate：$z_t = \operatorname*{sigmoid}(W_{xz}x_t + W_{hz}h_{t-1} + b_z)$；
- 重置门/Reset Gate：$r_t = \operatorname*{sigmoid}(W_{xr}x_t + W_{hr}h_{t-1} + b_r)$；
- 隐式编码输出：$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \operatorname*{tanh}(W_{xh}x_t + U_{h}(r_t \odot h_{t-1}) + b_h)$。

这里的模型参数 $W_{xz}$、$W_{hz}$、$b_z$、$W_{xr}$、$W_{hr}$、$b_r$、$W_{xh}$、$U_{h}$ 和 $b_h$ 都是需要训练优化。

<img class="center-picture" src="../assets/4-DL-GRU.png" alt="illustration" width="550" />

更新门类似于遗忘门与输入门，决定要忘记或者添加那些信息，控制前一状态的信息被保留到当前状态中的程度，更新门的值越大，前一状态的信息被保留的程度越高。重置门控制忽略前一时刻的状态信息的程度，重置门的信息越少，信息通过程度越低，信息被忽略的程度就越高。

### 6. 神经网络正则化

深度神经网络的结构复杂，参数多，容易过拟合，我们使用正则化技术提升神经网络的泛化能力。

- **Dropout**：在训练过程中，随机丢掉一部分神经元来降低神经网络的复杂度，从而防止过拟合。实现：通过每次迭代训练过程中，都以一定概率随机屏蔽每一层中的若干个神经元。
- **批归一化/Batch Normalization**：由于随着神经网络的深度增加，输入数据经过激活函数若干次非线性变换后，整体分布朝着非线性函数的值域上下界两端靠拢，因此在反向传播的过程中，靠近输入端处容易出现梯度消失问题。解决方法：通过规范化手段，把神经网络每一层中的任意神经元的输入值分布改变成均值为 0，方差为 1 的标准正态分布。
- **$L_1$ 和 $L_2$ 正则化**：对于具有 $n$ 个训练数据 $\{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}$ 的神经网络模型，加入**正则化项**之后的神经网络的损失函数一般可以表示如下：

    $$
    \min \frac{1}{n} \sum_{i = 1}^{n}\operatorname*{Loss}(y_i, f(\boldsymbol{W}, x_i)) + \lambda \times \Phi(\boldsymbol{W})
    $$

    其中 $f(\boldsymbol{W}, x_i)$ 是神经网络模型的输出，$\boldsymbol{W}$ 是神经网络模型的参数，$\lambda$ 是正则化权重，正则化项一般使用参数的范数来表示，常见的是 $L_0$ 范数（$L_0$ 正则化是一个 $\mathsf{NP}$ 难问题，所以用的少）、$L_1$ 范数（稀疏规则蒜子/Lasso Regularization）和 $L_2$ 范数。

### 7. 前沿应用

- 注意力机制：不想写了，这本书写的两页等于没写。
- Word2Vec：
- 图像分类与目标定位

## 强化学习

### 1. 强化学习基本概念

基本概念：

- **智能体/Agent**：强化学习算法的主体，根据经验做出主观判断并且执行动作；
- **环境/Environment**：智能体以外的一切都称为环境，环境在与智能体的交互中，能被智能体采取的动作影响，同时智能体也能向智能体反馈状态和奖励；
- **状态/State**：智能体对环境的一种理解和编码，包含对智能体采取决策产生影响的信息；
- **动作/Action**：智能体对环境产生影响的方式；
- **策略/Policy**：智能体在所处状态下执行某个动作的依据，即给定一个状态，智能体可以根据一个策略来选择应该采取的动作；
- **奖励/Reward**：智能体在环境中执行动作后，从环境中获得的收益。奖励是现实中奖励和惩罚的综合，用正值表示实际奖励，负值表示实际惩罚。

**强化学习是智能体在与环境交互中学习能够帮助其获得最大化奖励这一策略的过程**。在每一次迭代中，智能体根据当前策略选择一个动作，该动作影响环境，导致环境发生改变，智能体从环境中得到状态变化和奖励反馈等信息，并根据这些反馈更新其内部策略。

|  学习方式  |       学习依据       |       数据来源       |         决策过程         |                学习目标                |
| :--------: | :------------------: | :------------------: | :----------------------: | :------------------------------------: |
|  监督学习  |     基于监督信息     |  一次给定的标注数据  | 单步决策（分类和识别等） |          样本到语义标签的映射          |
| 无监督学习 | 基于对数据结构的假设 | 一次给定的未标注数据 |            无            |             数据的分布模式             |
|  强化学习  |       基于评估       |   在时序交互中产生   |         序贯决策         | 选择能够获取最大收益的状态到动作的映射 |

**马尔可夫决策过程/Markov Decision Process/MDP**：强化学习的基本数学模型，由一个五元组 $(S, A, P, R, \gamma)$ 组成，其中：

- $S$：状态集合，所求解问题所有可能出现的状态构成的集合，集合不保证有限；
- $A$：动作集合，所求解问题中智能体可以采取的所有动作构成的集合，集合不保证有限；
- $P$：状态转移概率分布，$P(s_{t+1} \mid s_t, a_t)$ 表示在状态 $s_t$ 采取动作 $a_t$ 后转移到状态 $s_{t+1}$ 的概率，其满足马尔可夫性。状态转移可以确定也可以随机；
- $R$：奖励函数，$R(s_t, a_t, s_{t+1})$ 表示在状态 $s_t$ 采取动作 $a_t$ 后转移到状态 $s_{t+1}$ 的奖励；
- $\gamma$：折扣因子，$0 \leq \gamma \leq 1$，后续时刻奖励对于当前动作的价值系数。

**形式化问题定义**：

### 2. 基于价值的强化学习

### 3. 基于策略的强化学习

### 4. 深度强化学习应用

## 人工智能博弈


