# Papers I Read

## Table of Contents

- [「KDD'2025」Achieving Nearly-Optimal Regret and Sample Complexity in Dueling Bandits with Applications in Online Recommendations](./1%20Dueling%20Bandits%20Near%20Optimal.md)
- [「JMLR'2006」Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems](./2%20Action%20Elimination%20for%20RL.md)
- [「ICML'2017」Dueling Bandits with Weak Regret](./3%20Dueling%20Bandits%20with%20Weak%20Regret.md)
- [「ICML'2011」Beat the Mean Bandit](./4%20Beat%20the%20Mean%20Bandit.md)
- [「ICML'2017」Maximum Selection and Ranking under Noisy Comparisons](./5%20Maximum%20Selection%20and%20Ranking%20under%20Noisy%20Comparisons.md)
- [Welcome to the Era of Experience](./6%20Era%20of%20Experience.md)

??? Info "原文目录"

    - [「KDD'2025」Achieving Nearly-Optimal Regret and Sample Complexity in Dueling Bandits with Applications in Online Recommendations](./assets_1/Achieving%20Nearly-Optimal%20Regret%20and%20Sample%20Complexity%20in%20Dueling%20Bandits%20with%20Applications%20in%20Online%20Recommendations.pdf)
    - [「JMLR'2006」Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems](./assets_2/Action%20Elimination%20and%20Stopping%20Conditions%20for%20the%20Multi-Armed%20Bandit%20and%20Reinforcement%20Learning%20Problems.pdf)
    - [「ICML'2017」Dueling Bandits with Weak Regret](./assets_3/Dueling%20Bandits%20with%20Weak%20Regret.pdf)
    - [「ICML'2011」Beat the Mean Bandit](https://www.cs.cornell.edu/people/tj/publications/yue_joachims_11a.pdf)
    - [「ICML'2017」Maximum Selection and Ranking under Noisy Comparisons](https://proceedings.mlr.press/v70/falahatgar17a.html)
    - [Welcome to the Era of Experience](https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf)

